# Q4.5 Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?

Case 1: If Training error is increasing with each epoch then the learning rate is too high.

Case 2: If Training error is NOT increasing with each epoch then we think we are overfitting, and we should stop training.

Early stopping p200
See Picture on p201
