# Q4.3 Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?

Will not get stuck because we are guaranteed to find the global minimum.  

Cost function J(\theta) is convex

"The good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough)."  p204

https://en.wikipedia.org/wiki/Convex_function
