# Q4.7 Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?

a) SGD reaches vincinity faster because using only one point is quicker. ie. calculating cost function is quicker.
b) Batch actually converges, but could take a while.
c) Reduce the learning rate
